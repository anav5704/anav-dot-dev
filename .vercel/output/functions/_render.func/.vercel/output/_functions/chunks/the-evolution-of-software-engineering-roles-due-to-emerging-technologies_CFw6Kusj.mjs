const id = "the-evolution-of-software-engineering-roles-due-to-emerging-technologies.md";
						const collection = "research";
						const slug = "the-evolution-of-software-engineering-roles-due-to-emerging-technologies";
						const body = "\r\nExtended Reality (XR) is a generalized term used to describe technologies that aim to combine the\r\nphysical and virtual world into one, seamless experience. Some of the popular technologies that fall\r\nunder this umbrella include Augmented Reality (AR), Virtual Reality (VR) and Mixed Reality (MR). XR\r\nis without a doubt an emerging technology as the current market size of $105.58 billion USD is\r\nexpected to grow to $472.3 billion USD by 2028 (Mordor Intelligence, 2023). In addition to this, the\r\nApple Vision Pro, a spatial computer that merges AR and VR, was announced early June this year.\r\n\r\n## The Impact of XR on Software Engineering Role\r\n\r\nThe rapid growth of advanced technology such as extended reality is bound to have various impacts\r\non software engineering roles. One of the major impacts is enhanced remote collaboration and\r\nbetter communication among programmers (A. Elliott, Peiris, & Parnin, 2015). Working remotely is\r\nstandardized in the software engineering field, and XR takes it a step further by letting developers\r\nbased in different geographical locations join each other in a virtual live coding environment. This\r\nalso promotes parallel development as there could be various teams (development, code review,\r\ndesign) working simultaneously.\r\n\r\nThat was a scenario where software engineers use XR to develop applications, but what if software\r\nengineers need to develop applications for XR? An impact of this would be that developers and\r\ndesigners would need to learn new skills and tools like 3d modelling, advanced animations, and\r\nspatial sound design to efficiently deliver functional software. Alongside this comes a vast number of\r\nchallenges such as creating immersive experiences and ensuring user safety.\r\n\r\n## The Evolution of Frontend Web Developer Responsibilities\r\n\r\nThe responsibility of Frontend Web Developer, at its simplest, is to make user friendly web pages\r\nusing HTML, CSS and JavaScript that are fast and responsive. In recent years, this role has seen\r\nvarious changes with the increased popularity of JavaScript frameworks (like React JS) and\r\ncomponents based / modular development.\r\nExtended reality evolves this role’s responsibility in two main ways. Firstly, to make websites that\r\nincorporate XR, frontend web developers need to learn and master new XR frameworks. Some of\r\nthese frameworks include WebXR, A-Frame and ReactVR (B. Alexey, 2021), which provides\r\ndevelopers with the necessary tools, libraries, and APIs to create XR content that can be accessed via\r\nweb browsers.\r\n\r\nSecondly, frontend web developers need to learn how to create and implement immersive UI/UX\r\ndesign. It involves considering components such as spatial mapping and 3D visualization which are\r\nused to anchor virtual objects or information to the user's physical space (Uxcel, 2023). Even more\r\nbasic interactions like text fields need to be redefined as now instead of a keyboard and muse, the\r\ninput methods are gestures, voice commands, and hand controllers.\r\n\r\n## Ethical Considerations and Challenges in XR\r\n\r\nDespite groups such as IEEE promoting increased focus on ethics (for XR and technologies in general),\r\nthere remains various ethical considerations (IEEE, 2023). The most obvious one being separation\r\nfrom the real world. Putting on an XR headset means blocking yourself from the physical world,\r\nwhich can become an issue when working among software engineering professionals. This limits the\r\nability for peers to ask questions and denies the presence of a physical learning environment.\r\nFurthermore, there are a handful of challenges associated with the integration of XR into\r\ndevelopment teams, with health concerns being at the top of the list. According to StackOverflow’s\r\nstudy of 41,151 respondents, the majority of the software engineers work 8-9 hours a day, of which\r\n2-5 hours is spent on coding (M. Elmar, 2023). Spending long hours in an XR headset can lead to\r\nnausea and dizziness. This is caused when the brain receives mixed signals i.e., the eyes register\r\ndigital signals, while your inner the knows that the body is in the real world.\r\n\r\n## Requirements Gathering / Planning\r\n\r\nIn this phase, meetings are held with stakeholders and information on the problem domain is\r\ngathered. With the emergence of XR technologies like immersive augmented reality (IAR),\r\nrequirements engineers can use this to better understand and respond to stakeholders (L. Merino,\r\n2020). In practice, the requirements engineers will talk to the stakeholders as per usual, but while\r\nthis is happening, the IAR headset will provide an interactive view of “nodes” or “entities” in real\r\ntime. This will in turn help the requirements engineers to `visualize the relationship between\r\nconcepts.\r\n\r\n## Requirements Analysis\r\n\r\nThe main purpose of the requirements analysis phase is to understand the information collected in\r\nthe previous phase and to plan the general approach that will be used in the development.\r\nComplementary to writing notes and analyzing data by drawing on whiteboards, requirements\r\nengineers can utilize IAR headsets to digitize the notes and diagrams (L. Merino, 2020). This allows\r\ndata to be organized in a spatial way and improves efficiency. In addition to this, large chunks of data\r\ncan be decomposed and related data can be bundled freely.\r\n\r\n## Design\r\n\r\nIn the design phase, the overall look, feel and functionality of the product is decided. The use of IAR\r\nin this phase can allow designers to project UML diagrams (like use case diagrams) to a wall (L.\r\nMerino, 2020). This lets designers add use cases by drawing boxes and linking them using lines with\r\nmakers, which function as a stylus. The software then takes these doodles and properly converts\r\nthem to use cases. Built in microphones can be used to give names to these use cases because of\r\nvoice recognition.\r\n\r\n## Coding / Implementation\r\n\r\nThe design from the previous phase is converted into code and executable programs in this phase.\r\nMixed reality can be used in many ways here, Firstly, developers can use IAR headsets and place\r\nrecently viewed files to the left of their desktop and documentations for needed technologies to the\r\non the right. This will drastically help boost productivity. Secondly, Remote pair programming (also\r\nknown as distributed or virtual pair programming) can be done (J. Dominic, 2020). The navigator and\r\ndriver can both use XR devices to work at a virtual desk from anywhere which promotes flexibility.\r\n\r\n## Testing\r\n\r\nIn this phase, software is assessed at various levels (component testing, integration testing) to ensure\r\nit meets the project standards. Similar to the design phase, testers can use IAR to virtually write and\r\ntest code on a whiteboard with syntax highlighting. “City visualization” health checks can also be\r\ndone using IAR (L. Merino, 2020). This is where buildings represent test classes and districts\r\nrepresent test suites. The lack of buildings in a district means that it lacks test coverage. The results Is\r\nan immersive and interactive way to test projects.\r\n\r\n## Deployment\r\n\r\nIn the deployment phase, the finished software is released and installed on user machines. Despite\r\nXR being more prominent in the other phases, it can still be used in deployment in niche ways like\r\nend user training. VR headsets could be used to replicate a virtual office and help prepare users for\r\nvarious expected, and unforeseen scenarios (S. Bennett, 2023). Such VR training simulators can\r\ninclude troubleshooting tasks whereby if the user fails to complete a task, a step-by-step guide will\r\nbe displayed in the spatial environment.\r\n\r\n## Maintenance\r\n\r\nSoftware is kept fully functional and newly discovered issues are fixed in this phase. Once again, the\r\nIAR headsets and whiteboard can be used here. In the case of fixing newly found issues, developers\r\ncan have build configurations for the software displayed spatially and work on debugging issues.\r\nCollaborative software will then use cameras to capture and update the new build configuration (L.\r\nMerino, 2020). Even in a software update scenario, developers can brainstorm ideas and solutions\r\nfaster. The whiteboard approach fosters a creative and collaborative experience.\r\n\r\n## Benefits of XR\r\n\r\nAs mentioned earlier, enhanced remote collaboration is one of the more significant benefits of using\r\nXR in SLDC. Platforms like skype offer connectedness but lack speed and instant messaging offers\r\nspeed but lacks connectedness (A. Elliott, 2015). XR technologies fit between and above these as\r\nthey offer speed, connectedness and add the third dimension of reality. This allows teams from\r\nseparate locations to work seamlessly in a shared virtual space.\r\nAnother benefit of XR is spatial data visualization. 2d data visualization, despite being easier to\r\nnavigate, can get very clustered as projects and the dataset grow. Having the third dimension in XR\r\nmeans enormous amounts of data can be seen and analyzed more effectively. Similar to the “city\r\nvisualization” mentioned in the testing phase, “Solar System visualization” (A. Schreiber, 2019) can be\r\nused. This is where packages are made into suns which have planets orbiting them. In this system,\r\nplanets are classes, orbits are inheritance levels, and the sizes of planets vary on the lines of code\r\n(LOC) for that class.\r\n\r\n## Potential Drawbacks of XR\r\n\r\nUnfortunately, just like with every other technology, there are bound to be some drawbacks that\r\nneed to be considered. The most obvious one being cost. The apple vison pro (which will be the most\r\nadvanced VR headset when it releases next year) will cost $3,499.00 USD. If XR technologies like this\r\nwould to be incorporated into software development, it would cost significantly more as it would\r\nneed to have features and applications tailored to SDLC.\r\nAnother drawback is accessibility, which can lead to the exclusion of users with disabilities. Due to\r\nthe immersive nature of XR, sensory disabilities, especially vision issues will have a massive impact\r\non the usage of the technology. AR and VR heavily rely on the user’s depth perception as it is\r\nresponsible for mentally mapping out the physical environment in 3d. If users lack depth perception,\r\nthe technology will no longer be immersive and thus there will be no point in using it.\r\n";
						const data = {title:"The Evolution of Software Engineering Roles Due to Emerging Technologies.",date:new Date(1584250507322),featured:true};
						const _internal = {
							type: 'content',
							filePath: "C:/Users/anavk/code/web/anav.dev/src/content/research/the-evolution-of-software-engineering-roles-due-to-emerging-technologies.md",
							rawData: undefined,
						};

export { _internal, body, collection, data, id, slug };
